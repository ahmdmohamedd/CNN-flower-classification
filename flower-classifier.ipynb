{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31bb3041-7e81-4fae-bd58-2db58197929b",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aedbea48-0cfd-44d7-abcd-caa7c258c858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2936 files for training.\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 734 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Download the flower dataset\n",
    "data_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "path_to_zip = keras.utils.get_file('flower_photos.tgz', origin=data_url, extract=True)\n",
    "base_dir = path_to_zip.replace('flower_photos.tgz', 'flower_photos')\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    base_dir,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    base_dir,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35215c2f-19cb-4cc5-a385-5fb1ec4f6e44",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f19ddf-d924-46aa-9931-83317ee456ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Apply normalization\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0462f7e1-8fba-489a-b1a7-5da54c2c7842",
   "metadata": {},
   "source": [
    "# Build the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feadb75b-eda1-4402-8054-9ab2082cb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(5, activation='softmax')  # 5 classes for the flowers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8522d4-f04a-415d-a5e0-9832c08f8e9e",
   "metadata": {},
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f7b53a-e8bd-4df7-8968-7c232e8963ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710883c7-f0d7-4440-b3f1-4c22e23e7f23",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3154c3e8-cb2d-42e7-94ec-f0e36a3948b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "92/92 [==============================] - 20s 202ms/step - loss: 1.2807 - accuracy: 0.4506 - val_loss: 1.0569 - val_accuracy: 0.5695\n",
      "Epoch 2/10\n",
      "92/92 [==============================] - 20s 218ms/step - loss: 0.9792 - accuracy: 0.6114 - val_loss: 0.9747 - val_accuracy: 0.6294\n",
      "Epoch 3/10\n",
      "92/92 [==============================] - 20s 219ms/step - loss: 0.8291 - accuracy: 0.6798 - val_loss: 0.8864 - val_accuracy: 0.6594\n",
      "Epoch 4/10\n",
      "92/92 [==============================] - 20s 218ms/step - loss: 0.7107 - accuracy: 0.7279 - val_loss: 0.8693 - val_accuracy: 0.6608\n",
      "Epoch 5/10\n",
      "92/92 [==============================] - 20s 220ms/step - loss: 0.5287 - accuracy: 0.8048 - val_loss: 0.8937 - val_accuracy: 0.6662\n",
      "Epoch 6/10\n",
      "92/92 [==============================] - 20s 220ms/step - loss: 0.3582 - accuracy: 0.8750 - val_loss: 0.9837 - val_accuracy: 0.6730\n",
      "Epoch 7/10\n",
      "92/92 [==============================] - 20s 217ms/step - loss: 0.2466 - accuracy: 0.9121 - val_loss: 1.3920 - val_accuracy: 0.6117\n",
      "Epoch 8/10\n",
      "92/92 [==============================] - 20s 217ms/step - loss: 0.1614 - accuracy: 0.9486 - val_loss: 1.2028 - val_accuracy: 0.6649\n",
      "Epoch 9/10\n",
      "92/92 [==============================] - 20s 219ms/step - loss: 0.0883 - accuracy: 0.9741 - val_loss: 1.5156 - val_accuracy: 0.6771\n",
      "Epoch 10/10\n",
      "92/92 [==============================] - 21s 220ms/step - loss: 0.0475 - accuracy: 0.9867 - val_loss: 1.5834 - val_accuracy: 0.7016\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=10, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a7f4d-8fc8-4ba7-8c03-95419ac91080",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2dc5616-a488-410b-8524-fd9f4994224d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 1s 48ms/step - loss: 1.5834 - accuracy: 0.7016\n",
      "Validation accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(val_ds)\n",
    "print(f\"Validation accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1346dd-d84c-4725-b1cf-024bd00e4568",
   "metadata": {},
   "source": [
    "# Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33119a4b-d3bc-4921-8031-c17691d5a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3145e07-cf6e-4fad-92e9-94655b76fb14",
   "metadata": {},
   "source": [
    "# Classify an external image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e73e89-57b6-4eae-8602-cfb774408cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess an external image\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(128, 128))  # Resize to match model input\n",
    "    img_array = image.img_to_array(img)  # Convert image to array\n",
    "    img_array = img_array / 255.0  # Normalize pixel values\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "# Function to classify an external image\n",
    "def classify_image(model, img_path):\n",
    "    processed_image = load_and_preprocess_image(img_path)\n",
    "    predictions = model.predict(processed_image)  # Get predictions\n",
    "    predicted_class = np.argmax(predictions[0])  # Get the index of the highest predicted value\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage\n",
    "img_path = 'path_to_your_image.jpg'  # Replace with your image path\n",
    "predicted_class = classify_image(model, img_path)\n",
    "print(f\"The predicted class is: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
